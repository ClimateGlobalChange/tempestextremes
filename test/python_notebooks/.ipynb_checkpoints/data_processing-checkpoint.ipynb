{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary modules\n",
    "import os\n",
    "#Some hacks to get around a Basemap issue\n",
    "os.environ[\"PYTHONWARNINGS\"]=\"ignore::yaml.YAMLLoadWarning\"\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import xarray as xa\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import nc_time_axis\n",
    "import cftime\n",
    "import scipy\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import kruskal\n",
    "from sklearn.decomposition import PCA\n",
    "from ordered_set import OrderedSet\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "dats_mod=[\"CFSR\",\"MERRA\",\"ERA\",\"JRA\"]\n",
    "#dats_mod2=[\"CMCC-CESM\",\"CMCC-CM\",\"CNRM-CM5\",\n",
    "#          \"CanESM2\",\"GFDL-CM3\",\"GFDL-ESM2M\",\n",
    "          #\"HadCM3\",\"HadGEM2-CC\",\"IPSL-CM5A\",\n",
    "#           \"IPSL-CM5A\",\n",
    "#          \"MIROC-ESM\",\"MIROC5\",\"MPI-ESM-MR\",\n",
    "#          \"MRI-CGCM3\",\"MRI-ESM1\",\"NorESM1-M\"]\n",
    "\n",
    "dats_mod2=[\"CMCC-CM\",\"MRI-CGCM3\",\"MRI-ESM1\",\n",
    "          \"MIROC5\",\"CNRM-CM5\",\"GFDL-ESM2M\",\"MPI-ESM-MR\",\n",
    "          \"NorESM1-M\",\"IPSL-CM5A\",\n",
    "          \"GFDL-CM3\",\"CanESM2\",\"MIROC-ESM\",\"CMCC-CESM\"]\n",
    "dats_all = dats_mod + dats_mod2\n",
    "\n",
    "dats_esm = [\"MRI-ESM1\",\"CNRM-CM5\",\"GFDL-ESM2M\",\n",
    "           \"MPI-ESM-MR\",\"NorESM1-M\",\"IPSL-CM5A\",\n",
    "           \"CanESM2\",\"MIROC-ESM\",\"CMCC-CESM\"]\n",
    "dats_aog = [\"CMCC-CM\",\"MRI-CGCM3\",\"MIROC5\",\"GFDL-CM3\"]\n",
    "\n",
    "dats_cmip6=[\"EC-Earth3\",\"CESM2\",\"CESM2-WACCM\",\n",
    "            \"BCC-CSM2-MR\",\"MRI-ESM2-0\",\n",
    "           \"IPSL-CM6A-LR\",\"GFDL-CM4\",\"GISS-E2-1-G\",\n",
    "           \"CanESM5\",\"BCC-ESM1\"]\n",
    "\n",
    "base_dir=\"/global/cscratch1/sd/marielp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Make a function to calculate the seasonal means for blocking frequency\n",
    "\n",
    "def read_files_make_mean(season_str,dataset_rean,dataset_model,thresh,base_dir_rean,base_dir_mod):\n",
    "    #Examine the blocking frequency\n",
    "    flist_blobs={}\n",
    "    for d in dataset_rean:\n",
    "        f_glob=sorted(glob.glob(\"{:}/{:}/BLOBS_NOREGIONAL/*{:}_blobs_noregional.nc\".format(base_dir_rean,d,season_str)))\n",
    "        flist_blobs[d] = f_glob\n",
    "        #Examine the blocking frequency\n",
    "    flist_blobs2={}\n",
    "    for d in dataset_model:\n",
    "        f_glob=sorted(glob.glob(\"{:}/{:}/BLOBS_NOREGIONAL/*{:}_blobs_noregional.nc\".format(base_dir_mod,d,season_str)))\n",
    "        flist_blobs2[d] = f_glob\n",
    "    new_lons = np.arange(0.,360.)\n",
    "    new_lats=np.arange(-90.,91.)\n",
    "    model_avg=xa.Dataset(coords={\"lat\":new_lats,\"lon\":new_lons})\n",
    "    model_avg['reanBlockMean'] = xa.DataArray(np.zeros((len(new_lats),len(new_lons))),dims=[\"lat\",\"lon\"])\n",
    "    model_avg['modelBlockMean'] = xa.DataArray(np.zeros((len(new_lats),len(new_lons))),dims=[\"lat\",\"lon\"])\n",
    "    model_avg['coswgt']=xa.DataArray(np.cos( model_avg['lat'].values *math.pi / 180.),dims='lat')\n",
    "    \n",
    "    n_rean=len(dataset_rean)\n",
    "    mod_res={}\n",
    "\n",
    "    for d in dataset_rean:\n",
    "        res_temp={}\n",
    "        dcurr=flist_blobs[d]\n",
    "        d_mf = xa.open_mfdataset(dcurr)\n",
    "        res_temp['lat']=d_mf['lat'].values[1]-d_mf['lat'].values[0]\n",
    "        res_temp['lon']=d_mf['lon'].values[1]-d_mf['lon'].values[0]\n",
    "        mod_res[d]=res_temp\n",
    "        b_avg = d_mf['Z_BLOB'].mean(dim='time')\n",
    "        b_avg_interp = b_avg.interp(lat=new_lats,lon=new_lons).fillna(0)\n",
    "        b_wgt_val = b_avg_interp*model_avg['coswgt'].values[:,np.newaxis]\n",
    "        model_avg['reanBlockMean']+=(b_avg_interp.values/float(n_rean))\n",
    "        model_avg[d] = b_avg_interp\n",
    "        model_avg['{:}_WGT'.format(d)] = b_wgt_val\n",
    "\n",
    "    model_avg['reanBlockMean_WGT'] = model_avg['reanBlockMean'] * model_avg['coswgt'].values[:,np.newaxis]\n",
    "    \n",
    "    n_mod=len(dataset_model)\n",
    "    mod_res2={}\n",
    "    for d in dataset_model:\n",
    "        res_temp={}\n",
    "        dcurr=flist_blobs2[d]\n",
    "        d_mf = xa.open_mfdataset(dcurr)\n",
    "        res_temp['lat']=d_mf['lat'].values[1]-d_mf['lat'].values[0]\n",
    "        res_temp['lon']=d_mf['lon'].values[1]-d_mf['lon'].values[0]\n",
    "        mod_res2[d]=res_temp\n",
    "        b_avg = d_mf['Z_BLOB'].mean(dim='time')\n",
    "        b_avg_interp = b_avg.interp(lat=new_lats,lon=new_lons).fillna(0)\n",
    "        b_wgt_val = b_avg_interp*model_avg['coswgt'].values[:,np.newaxis]\n",
    "        model_avg['modelBlockMean']+=(b_avg_interp.values/float(n_mod))\n",
    "        mod_diff = b_avg_interp.values - model_avg['reanBlockMean'].values\n",
    "        \n",
    "        model_avg['{:}_diff_num'.format(d)] = xa.Variable(('lat','lon'),np.ndarray.copy(mod_diff))\n",
    "        threshold_indices = np.absolute(mod_diff) < thresh\n",
    "        mod_diff[threshold_indices] = np.nan\n",
    "        model_avg[d] = b_avg_interp\n",
    "        model_avg['{:}_diff'.format(d)] = xa.Variable(('lat','lon'),mod_diff)\n",
    "        model_avg['{:}_WGT'.format(d)] = b_wgt_val\n",
    "\n",
    "    model_avg['modelBlockMean_WGT'] = model_avg['modelBlockMean'] * model_avg['coswgt'].values[:,np.newaxis] \n",
    "    diff_vals = model_avg['modelBlockMean'].values-model_avg['reanBlockMean'].values\n",
    "    model_avg['modelDiff_num'] = xa.Variable(('lat','lon'),np.ndarray.copy(diff_vals))\n",
    "    threshold_indices = np.absolute(diff_vals) < thresh\n",
    "    diff_vals[threshold_indices] = np.nan\n",
    "    \n",
    "    model_avg['modelDiff'] = xa.Variable(('lat','lon'),diff_vals)\n",
    "    return(model_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEASONAL MEANS OF BLOCKING FREQUENCY\n",
    "avg_djf = read_files_make_mean(\"DJF\",dats_mod, dats_mod2,0.005,base_dir,base_dir)\n",
    "#avg_djf.to_netcdf('{:}/REAN_MODEL_MEANS_DJF.nc'.format(base_dir))\n",
    "\n",
    "avg_jja = read_files_make_mean(\"JJA\",dats_mod, dats_mod2,0.005,base_dir,base_dir)\n",
    "#avg_jja.to_netcdf('{:}/REAN_MODEL_MEANS_JJA.nc'.format(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More general form\n",
    "def read_files_var_mean(flist_rean,flist_mod,vname_r,vname_m,\n",
    "                        dataset_rean,dataset_model,base_dir_rean,base_dir_mod,\n",
    "                        decode_time=True,day_start=0,day_end=364,plev_r=0,plev_m=0,plev_r_name=\"plev\",plev_m_name=\"plev\"):\n",
    "    #Examine the blocking frequency\n",
    "\n",
    "    new_lons = np.arange(0.,360.)\n",
    "    new_lats=np.arange(-90.,91.)\n",
    "    model_avg=xa.Dataset(coords={\"lat\":new_lats,\"lon\":new_lons})\n",
    "    model_avg['reanMean'] = xa.DataArray(np.zeros((len(new_lats),len(new_lons))),dims=[\"lat\",\"lon\"])\n",
    "    model_avg['modelMean'] = xa.DataArray(np.zeros((len(new_lats),len(new_lons))),dims=[\"lat\",\"lon\"])\n",
    "    model_avg['coswgt']=xa.DataArray(np.cos( model_avg['lat'].values *math.pi / 180.),dims='lat')\n",
    "    n_rean=len(dataset_rean)\n",
    "    mod_res={}\n",
    "\n",
    "\n",
    "    for d in dataset_rean:\n",
    "        res_temp={}\n",
    "        dcurr=flist_rean[d]\n",
    "        d_mf = xa.open_mfdataset(dcurr,decode_times=decode_time)\n",
    "        res_temp['lat']=d_mf['lat'].values[1]-d_mf['lat'].values[0]\n",
    "        res_temp['lon']=d_mf['lon'].values[1]-d_mf['lon'].values[0]\n",
    "        plev_ind=-9999\n",
    "        if (plev_r!=0):\n",
    "            #Find the proper level for the data\n",
    "            plev_ind=np.where(d_mf[plev_r_name].values==plev_r)[0][0]\n",
    "        mod_res[d]=res_temp\n",
    "        if (day_start!=0 | day_end!=364):\n",
    "            if (day_end<day_start):\n",
    "                var_slice=d_mf[vname_r].sel(time=((d_mf['time']>=day_start) | (d_mf['time']<=day_end)))\n",
    "            else:\n",
    "                var_slice=d_mf[vname_r].sel(time=slice(day_start,day_end))\n",
    "        else:\n",
    "            var_slice=d_mf[vname_r]\n",
    "        if (plev_ind!=-9999):\n",
    "            var_slice=var_slice[:,plev_ind,:,:]\n",
    "        b_avg = var_slice.mean(dim='time')\n",
    "        b_avg_interp = b_avg.interp(lat=new_lats,lon=new_lons).fillna(0)\n",
    "        b_wgt_val = b_avg_interp*model_avg['coswgt'].values[:,np.newaxis]\n",
    "        model_avg['reanMean']+=(b_avg_interp.values/float(n_rean))\n",
    "        model_avg[d] = b_avg_interp\n",
    "        model_avg['{:}_WGT'.format(d)] = b_wgt_val\n",
    "\n",
    "    model_avg['reanMean_WGT'] = model_avg['reanMean'] * model_avg['coswgt'].values[:,np.newaxis]   \n",
    "    \n",
    "    n_mod=len(dataset_model)\n",
    "    mod_res2={}\n",
    "    for d in dataset_model:\n",
    "        res_temp={}\n",
    "        dcurr=flist_mod[d]\n",
    "        d_mf = xa.open_mfdataset(dcurr,decode_times=decode_time)\n",
    "        res_temp['lat']=d_mf['lat'].values[1]-d_mf['lat'].values[0]\n",
    "        res_temp['lon']=d_mf['lon'].values[1]-d_mf['lon'].values[0]\n",
    "        mod_res2[d]=res_temp\n",
    "        plev_ind=-9999\n",
    "        if (plev_m!=0):\n",
    "            #Find the proper level for the data\n",
    "            plev_ind=np.where(d_mf[plev_m_name].values==plev_m)[0][0]\n",
    "        if (day_start!=0 | day_end!=364):\n",
    "            if (day_end<day_start):\n",
    "                var_slice=d_mf[vname_m].sel(time=((d_mf['time']>=day_start) | (d_mf['time']<=day_end)))\n",
    "            else:\n",
    "                var_slice=d_mf[vname_m].sel(time=slice(day_start,day_end))\n",
    "        else:\n",
    "            var_slice=d_mf[vname_m]\n",
    "        if (plev_ind!=-9999):\n",
    "            var_slice=var_slice[:,plev_ind,:,:]\n",
    "        b_avg = var_slice.mean(dim='time')\n",
    "        b_avg_interp = b_avg.interp(lat=new_lats,lon=new_lons).fillna(0)\n",
    "        b_wgt_val = b_avg_interp*model_avg['coswgt'].values[:,np.newaxis]\n",
    "        model_avg['modelMean']+=(b_avg_interp.values/float(n_mod))\n",
    "        model_avg[d] = b_avg_interp\n",
    "        model_avg['{:}_WGT'.format(d)] = b_wgt_val\n",
    "\n",
    "    model_avg['modelMean_WGT'] = model_avg['modelMean'] * model_avg['coswgt'].values[:,np.newaxis] \n",
    "\n",
    "    return(model_avg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist_rean={}\n",
    "for d in dats_mod:\n",
    "    f_glob=sorted(glob.glob(\"{:}/{:}/*_avg.nc\".format(base_dir,d)))\n",
    "    flist_rean[d] = f_glob\n",
    "    #Examine the blocking frequency\n",
    "flist_mod={}\n",
    "for d in dats_mod2:\n",
    "    f_glob=sorted(glob.glob(\"{:}/{:}/*_avg.nc\".format(base_dir,d)))\n",
    "    flist_mod[d] = f_glob \n",
    "\n",
    "#LTDM\n",
    "#jja_az=read_files_var_mean(flist_rean,flist_mod,'AZ500',dats_mod,dats_mod2,base_dir,base_dir,decode_time=False,day_start=151,day_end=243)\n",
    "#jja_az.to_netcdf('{:}/REAN_MODEL_MEAN_AZ500_JJA.nc'.format(base_dir))\n",
    "#djf_az=read_files_var_mean(flist_rean,flist_mod,'AZ500',dats_mod,dats_mod2,base_dir,base_dir,decode_time=False,day_start=334,day_end=58)\n",
    "#djf_az.to_netcdf('{:}/REAN_MODEL_MEAN_AZ500_DJF.nc'.format(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z500, JJA\n",
    "flist_mod={}\n",
    "flist_rean={}\n",
    "for d in dats_mod2:\n",
    "    flist_temp=[]\n",
    "    for y in range(1980,2006):\n",
    "        f_glob=glob.glob(\"{:}/{:}/zg*{:}0[678].nc\".format(base_dir,d,y))\n",
    "        flist_temp+=f_glob\n",
    "    flist_mod[d]=sorted(flist_temp)\n",
    "for d in dats_mod:\n",
    "    flist_temp=[]\n",
    "    for y in range(1980,2006):\n",
    "        f_glob=glob.glob(\"{:}/{:}/*{:}0[678]*dailyavg.nc\".format(base_dir,d,y))\n",
    "        flist_temp+=f_glob\n",
    "    flist_rean[d]=flist_temp\n",
    "\n",
    "#Brute force for ERA:\n",
    "flist_temp=[]\n",
    "for y in range(1980,2006):\n",
    "    f_glob=glob.glob(\"{:}/ERA/ERA_{:}_0[678]_*dailyavg.nc\".format(base_dir,y))\n",
    "    flist_temp+=f_glob\n",
    "flist_rean['ERA']=sorted(flist_temp)\n",
    "\n",
    "#jja_zg=read_files_var_mean(flist_rean,flist_mod,'Z','zg',dats_mod,dats_mod2,base_dir,base_dir,plev_m=50000)\n",
    "#jja_zg.to_netcdf('{:}/REAN_MODEL_MEAN_Z500_JJA.nc'.format(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z500, DJF\n",
    "flist_mod={}\n",
    "flist_rean={}\n",
    "for d in dats_mod2:\n",
    "    flist_temp=[]\n",
    "    for y in range(1980,2006):\n",
    "        f_glob=glob.glob(\"{:}/{:}/zg*{:}12.nc\".format(base_dir,d,y))\n",
    "        flist_temp+=f_glob\n",
    "        f_glob=glob.glob(\"{:}/{:}/zg*{:}0[12].nc\".format(base_dir,d,y))\n",
    "        flist_temp+=f_glob\n",
    "    flist_mod[d]=sorted(flist_temp)\n",
    "for d in dats_mod:\n",
    "    flist_temp=[]\n",
    "    for y in range(1980,2006):\n",
    "        f_glob=glob.glob(\"{:}/{:}/*{:}12*dailyavg.nc\".format(base_dir,d,y))\n",
    "        flist_temp+=f_glob\n",
    "        f_glob=glob.glob(\"{:}/{:}/*{:}0[12]*dailyavg.nc\".format(base_dir,d,y))\n",
    "        flist_temp+=f_glob\n",
    "    flist_rean[d]=flist_temp\n",
    "\n",
    "#Brute force for ERA:\n",
    "flist_temp=[]\n",
    "for y in range(1980,2006):\n",
    "    f_glob=glob.glob(\"{:}/ERA/ERA_{:}_12_*dailyavg.nc\".format(base_dir,y))\n",
    "    flist_temp+=f_glob\n",
    "    f_glob=glob.glob(\"{:}/ERA/ERA_{:}_0[12]_*dailyavg.nc\".format(base_dir,y))\n",
    "    flist_temp+=f_glob\n",
    "flist_rean['ERA']=sorted(flist_temp)\n",
    "\n",
    "#djf_zg=read_files_var_mean(flist_rean,flist_mod,'Z','zg',dats_mod,dats_mod2,base_dir,base_dir,plev_m=50000)\n",
    "#djf_zg.to_netcdf('{:}/REAN_MODEL_MEAN_Z500_DJF.nc'.format(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_seasonal_means(season_str, dataset_rean, dataset_model,year_arr,base_dir_rean,base_dir_model):\n",
    "    #Create the dataset\n",
    "    new_lons = np.arange(0.,360.)\n",
    "    new_lats=np.arange(-90.,91.)\n",
    "    model_month_means=xa.Dataset(coords={\"lat\":new_lats,\n",
    "                                          \"lon\":new_lons,\n",
    "                                          \"time\":year_arr})\n",
    "    for d in dataset_rean:\n",
    "        #Create a variable\n",
    "        v='{:}_SEASON_MEAN'.format(d)\n",
    "        model_month_means[v]=xa.DataArray(np.zeros((len(year_arr),len(new_lats),len(new_lons))),dims=[\"time\",\"lat\",\"lon\"])\n",
    "        for y in year_arr:\n",
    "            #Open up the relevant seasonal file\n",
    "            f=\"{:}/{:}/BLOBS_NOREGIONAL/{:}_{:}_{:}_blobs_noregional.nc\".format(base_dir_rean,d,d,y,season_str)    \n",
    "            d_mf=xa.open_dataset(f)\n",
    "            #Take the mean for the season\n",
    "            s_avg = d_mf['Z_BLOB'].mean(dim='time')\n",
    "            s_avg_interp=s_avg.interp(lat=new_lats,lon=new_lons).fillna(0)\n",
    "            model_month_means[v].loc[y,:,:]=s_avg_interp\n",
    "    #Find the reanalysis mean\n",
    "    n_rean=len(dataset_rean)\n",
    "    model_month_means['reanSeasonMean']=xa.DataArray(np.zeros((len(year_arr),len(new_lats),len(new_lons))),dims=[\"time\",\"lat\",\"lon\"])\n",
    "    for d in dataset_rean:\n",
    "        model_month_means['reanSeasonMean']+=model_month_means['{:}_SEASON_MEAN'.format(d)].values/float(n_rean)\n",
    "    for d in dataset_model:\n",
    "        #Create a variable\n",
    "        v='{:}_SEASON_MEAN'.format(d)\n",
    "        model_month_means[v]=xa.DataArray(np.zeros((len(year_arr),len(new_lats),len(new_lons))),dims=[\"time\",\"lat\",\"lon\"])\n",
    "        for y in year_arr:\n",
    "            #Open up the relevant seasonal file\n",
    "            f=\"{:}/{:}/BLOBS_NOREGIONAL/{:}_{:}_{:}_blobs_noregional.nc\".format(base_dir_model,d,d,y,season_str)    \n",
    "            d_mf=xa.open_dataset(f)\n",
    "            #Take the mean for the season\n",
    "            s_avg = d_mf['Z_BLOB'].mean(dim='time')\n",
    "            s_avg_interp=s_avg.interp(lat=new_lats,lon=new_lons).fillna(0)\n",
    "            model_month_means[v].loc[y,:,:]=s_avg_interp\n",
    "    n_model=len(dataset_model)\n",
    "    model_month_means['modelSeasonMean']=xa.DataArray(np.zeros((len(year_arr),len(new_lats),len(new_lons))),dims=[\"time\",\"lat\",\"lon\"])\n",
    "    for d in dataset_model:\n",
    "        model_month_means['modelSeasonMean']+=model_month_means['{:}_SEASON_MEAN'.format(d)].values/float(n_model)\n",
    "    return(model_month_means)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_file_seasonal_means() missing 2 required positional arguments: 'base_dir_rean' and 'base_dir_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6cda05cea2fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0myear_arr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1980\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdjf_means\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_file_seasonal_means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DJF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdats_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdats_mod2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myear_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdjf_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:}/REAN_MODEL_SEASON_MEANS_DJF.nc'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mjja_means\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_file_seasonal_means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"JJA\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdats_mod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdats_mod2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myear_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mjja_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:}/REAN_MODEL_SEASON_MEANS_JJA.nc'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: read_file_seasonal_means() missing 2 required positional arguments: 'base_dir_rean' and 'base_dir_model'"
     ]
    }
   ],
   "source": [
    "year_arr=np.arange(1980,2005)\n",
    "#djf_means=read_file_seasonal_means(\"DJF\", dats_mod, dats_mod2,year_arr,base_dir,base_dir) \n",
    "#djf_means.to_netcdf('{:}/REAN_MODEL_SEASON_MEANS_DJF.nc'.format(base_dir))\n",
    "#jja_means=read_file_seasonal_means(\"JJA\",dats_mod,dats_mod2,year_arr,base_dir,base_dir)\n",
    "#jja_means.to_netcdf('{:}/REAN_MODEL_SEASON_MEANS_JJA.nc'.format(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_djf=xa.open_dataset('{:}/REAN_MODEL_SEASON_MEANS_DJF.nc'.format(base_dir))\n",
    "month_jja=xa.open_dataset('{:}/REAN_MODEL_SEASON_MEANS_JJA.nc'.format(base_dir))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "#Calculate a matrix of difference significance\n",
    "def sigtest(a,b,rean,mod):\n",
    "    sig=ttest_ind(rean,mod,equal_var=False)\n",
    "    pvalue=sig.pvalue\n",
    "\n",
    "    return(pvalue)\n",
    "\n",
    "def sigmat_calc(rean,model):\n",
    "    s=rean.shape\n",
    "    sigmat=np.ones(s[1:3])\n",
    "    new_lons = np.arange(0.,360.)\n",
    "    new_lats=np.arange(-90.,91.)\n",
    "    \n",
    "    for a in range(0,len(new_lats)):\n",
    "        for b in range(0,len(new_lons)):\n",
    "            if ((np.abs(new_lats[a])>27)&(np.abs(new_lats[a])<73)):\n",
    "                rslice=rean.values[:,a,b]\n",
    "                mslice=model.values[:,a,b]\n",
    "                sumcheck=rslice+mslice\n",
    "                if(np.sum(sumcheck)>0):\n",
    "                    sigmat[a,b]=sigtest(a,b,rslice,mslice)\n",
    "        print(\"finished latitude {:}\".format(a))\n",
    "    return(sigmat)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lons = np.arange(0.,360.)\n",
    "new_lats=np.arange(-90.,91.)\n",
    "sig_freq_djf=xa.Dataset(coords={\"lat\":new_lats,\n",
    "                                      \"lon\":new_lons})\n",
    "\n",
    "sig_freq_djf['modelSig']=xa.DataArray(sigmat_calc(month_djf['reanSeasonMean'],month_djf['modelSeasonMean']),dims=['lat','lon'])\n",
    "for d in dats_mod:\n",
    "    sig_freq_djf['{:}Sig'.format(d)]=xa.DataArray(sigmat_calc(month_djf['reanSeasonMean'],month_djf['{:}_SEASON_MEAN'.format(d)]),dims=['lat','lon'])\n",
    "for d in dats_mod2:\n",
    "    sig_freq_djf['{:}Sig'.format(d)]=xa.DataArray(sigmat_calc(month_djf['reanSeasonMean'],month_djf['{:}_SEASON_MEAN'.format(d)]),dims=['lat','lon'])\n",
    "\n",
    "                                                  \n",
    "                                                  \n",
    "                                                \n",
    "sig_freq_jja=xa.Dataset(coords={\"lat\":new_lats,\n",
    "                                      \"lon\":new_lons})\n",
    "\n",
    "sig_freq_jja['modelSig']=xa.DataArray(sigmat_calc(month_jja['reanSeasonMean'],month_jja['modelSeasonMean']),dims=['lat','lon'])\n",
    "for d in dats_mod:\n",
    "    sig_freq_jja['{:}Sig'.format(d)]=xa.DataArray(sigmat_calc(month_jja['reanSeasonMean'],month_jja['{:}_SEASON_MEAN'.format(d)]),dims=['lat','lon'])\n",
    "for d in dats_mod2:\n",
    "    sig_freq_jja['{:}Sig'.format(d)]=xa.DataArray(sigmat_calc(month_jja['reanSeasonMean'],month_jja['{:}_SEASON_MEAN'.format(d)]),dims=['lat','lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sig_freq_djf.to_netcdf('{:}/REAN_MODEL_SIG_DJF.nc'.format(base_dir))\n",
    "#sig_freq_jja.to_netcdf('{:}/REAN_MODEL_SIG_JJA.nc'.format(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist_rean={}\n",
    "for d in dats_mod:\n",
    "    f_glob=sorted(glob.glob(\"{:}/{:}/*_avg.nc\".format(base_dir,d)))\n",
    "    flist_rean[d] = f_glob\n",
    "    #Examine the blocking frequency\n",
    "flist_mod={}\n",
    "for d in dats_mod2:\n",
    "    f_glob=sorted(glob.glob(\"{:}/{:}/*_avg.nc\".format(base_dir,d)))\n",
    "    flist_mod[d] = f_glob \n",
    "\n",
    "#Need to do a sigmat for LTDM\n",
    "#Just average everything together for daily values\n",
    "new_lons = np.arange(0.,360.)\n",
    "new_lats=np.arange(-90.,91.)\n",
    "new_time=np.arange(0,365)\n",
    "ltdm_avg=xa.Dataset(coords={\"time\":new_time,\"lat\":new_lats,\"lon\":new_lons})\n",
    "ltdm_avg['reanDailyAvg'] = xa.DataArray(np.zeros((len(new_time),len(new_lats),len(new_lons))),dims=[\"time\",\"lat\",\"lon\"])\n",
    "ltdm_avg['modelDailyAvg'] = xa.DataArray(np.zeros((len(new_time),len(new_lats),len(new_lons))),dims=[\"time\",\"lat\",\"lon\"])\n",
    "for d in dats_mod:\n",
    "    dcurr=flist_rean[d]\n",
    "    d_mf = xa.open_mfdataset(dcurr,decode_times=False)\n",
    "    var_slice=d_mf['AZ500']\n",
    "    v_interp=var_slice.interp(lat=new_lats,lon=new_lons).fillna(0)\n",
    "    ltdm_avg['reanDailyAvg']+=v_interp.values/float(len(dats_mod))\n",
    "for d in dats_mod2:\n",
    "    dcurr=flist_mod[d]\n",
    "    d_mf = xa.open_mfdataset(dcurr,decode_times=False)\n",
    "    var_slice=d_mf['AZ500']\n",
    "    v_interp=var_slice.interp(lat=new_lats,lon=new_lons).fillna(0)\n",
    "    ltdm_avg['modelDailyAvg']+=v_interp.values/float(len(dats_mod2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened dataset CMCC-CM\n",
      "Concatenated December-Feb\n",
      "finished latitude 0\n",
      "finished latitude 1\n",
      "finished latitude 2\n",
      "finished latitude 3\n",
      "finished latitude 4\n",
      "finished latitude 5\n",
      "finished latitude 6\n",
      "finished latitude 7\n",
      "finished latitude 8\n",
      "finished latitude 9\n",
      "finished latitude 10\n",
      "finished latitude 11\n",
      "finished latitude 12\n",
      "finished latitude 13\n",
      "finished latitude 14\n",
      "finished latitude 15\n",
      "finished latitude 16\n",
      "finished latitude 17\n",
      "finished latitude 18\n",
      "finished latitude 19\n",
      "finished latitude 20\n",
      "finished latitude 21\n",
      "finished latitude 22\n",
      "finished latitude 23\n",
      "finished latitude 24\n",
      "finished latitude 25\n",
      "finished latitude 26\n",
      "finished latitude 27\n",
      "finished latitude 28\n",
      "finished latitude 29\n",
      "finished latitude 30\n",
      "finished latitude 31\n",
      "finished latitude 32\n",
      "finished latitude 33\n",
      "finished latitude 34\n",
      "finished latitude 35\n",
      "finished latitude 36\n",
      "finished latitude 37\n",
      "finished latitude 38\n",
      "finished latitude 39\n",
      "finished latitude 40\n",
      "finished latitude 41\n",
      "finished latitude 42\n",
      "finished latitude 43\n"
     ]
    }
   ],
   "source": [
    "#Now make the sigmat for LTDM\n",
    "ltdm_sig_djf=xa.Dataset(coords={\"lat\":new_lats,\"lon\":new_lons})\n",
    "ltdm_sig_jja=xa.Dataset(coords={\"lat\":new_lats,\"lon\":new_lons})\n",
    "#Do the reanalysis slice for Dec-Feb\n",
    "slice_dec=ltdm_avg.sel(time=(ltdm_avg['time']>=334))\n",
    "slice_ja=ltdm_avg.sel(time=(ltdm_avg['time']<=58))\n",
    "slice_djf=xa.concat([slice_dec,slice_ja],dim='time')\n",
    "\n",
    "slice_jja=ltdm_avg.sel(time=((ltdm_avg['time']>=151) | (ltdm_avg['time']<=243)))\n",
    "\n",
    "for d in dats_mod2:\n",
    "    dcurr=flist_mod[d]\n",
    "    d_mf=xa.open_mfdataset(dcurr,decode_times=False)\n",
    "    print(\"Opened dataset {:}\".format(d))\n",
    "    v=d_mf['AZ500'].interp(lat=new_lats,lon=new_lons).fillna(0)\n",
    "    d_dec=v.sel(time=(v['time']>=334))\n",
    "    d_ja=v.sel(time=(v['time']<=58))\n",
    "    d_djf=xa.concat([d_dec,d_ja],dim='time')\n",
    "    print(\"Concatenated December-Feb\")\n",
    "    print(sigmat_calc(slice_djf['reanDailyAvg'],d_djf))\n",
    "    print('FInished sigmat calc')\n",
    "    \n",
    "    #ltdm_sig_djf['{:}Sig'.format(d)]=xa.DataArray(sigmat_calc(slice_djf['reanDailyAvg'],d_djf),dims=['lat','lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does monthly means but not using in this instance. Saving for reference!\n",
    "\n",
    "# \n",
    "# def read_file_monthly_means(season_str, dataset_rean, dataset_model):\n",
    "#     base_dir=\"/global/cscratch1/sd/marielp\"\n",
    "#     #Examine the blocking frequency\n",
    "#     flist_blobs={}\n",
    "#     for d in dataset_rean:\n",
    "#         f_glob=sorted(glob.glob(\"{:}/{:}/BLOBS_NOREGIONAL/*{:}_blobs_noregional.nc\".format(base_dir,d,season_str)))\n",
    "#         flist_blobs[d] = f_glob\n",
    "#         #Examine the blocking frequency\n",
    "#     flist_blobs2={}\n",
    "#     for d in dataset_model:\n",
    "#         f_glob=sorted(glob.glob(\"{:}/{:}/BLOBS_NOREGIONAL/*{:}_blobs_noregional.nc\".format(base_dir,d,season_str)))\n",
    "#         flist_blobs2[d] = f_glob\n",
    "#     #Create the monthly mean dataset\n",
    "#     new_lons = np.arange(0.,360.)\n",
    "#     new_lats=np.arange(-90.,91.)\n",
    "#     d_mf=xa.open_mfdataset(flist_blobs2[d])\n",
    "#     #Get lists of available years and months\n",
    "#     list_years=list(map(str,d_mf.time.dt.year.values))\n",
    "#     list_months=list(map('{:02d}'.format,d_mf.time.dt.month.values))\n",
    "#     unique_times=OrderedSet(list(zip(list_years,list_months)))\n",
    "#     time_list=['-'.join(i) for i in unique_times]\n",
    "    \n",
    "#     model_month_means=xa.Dataset(coords={\"lat\":new_lats,\n",
    "#                                          \"lon\":new_lons,\n",
    "#                                          \"time\":time_list})\n",
    "    \n",
    "#     for d in dataset_rean:\n",
    "#         dcurr=flist_blobs[d]\n",
    "#         d_mf = xa.open_mfdataset(dcurr)\n",
    "#         #Create a new blank variable\n",
    "#         #Create blank numpy array of \n",
    "#         mean_arr=np.zeros((len(time_list),len(new_lats),len(new_lons)))\n",
    "#         #String name \n",
    "#         v='{:}_MONTH_MEANS'.format(d)\n",
    "#         model_month_means[v]=xa.DataArray(mean_arr,dims=['time','lat','lon'])\n",
    "#         #Group the variable by months\n",
    "#         xtest=d_mf['Z_BLOB'].groupby('time.month')\n",
    "#         #Mean for the month in each of the years available\n",
    "#         for t in xtest.groups.keys():\n",
    "#             xyear=d_mf['Z_BLOB'][xtest.groups[t],:,:]\n",
    "#             yearmean=xyear.groupby('time.year').mean('time')\n",
    "#             yearmean_i=yearmean.interp(lat=new_lats,lon=new_lons).fillna(0)\n",
    "#             #Match to the correct time index by year and month\n",
    "#             yearlist=list(map(str,yearmean.year.values))\n",
    "#             inds=[y + '-{:02d}'.format(t) for y in yearlist]\n",
    "#             yearfin=xa.DataArray(yearmean_i.values,\n",
    "#                              coords={'lat':new_lats,\n",
    "#                                      'lon':new_lons,\n",
    "#                                      'time':inds},\n",
    "#                              dims=['time','lat','lon'])\n",
    "#             model_month_means[v].loc[inds,:,:]=yearfin     \n",
    "#     model_month_means['reanMonthMean'] = xa.DataArray(np.zeros(mean_arr.shape),dims=[\"time\",\"lat\",\"lon\"])\n",
    "#     n_rean=len(dataset_rean)\n",
    "#     for d in dataset_rean:\n",
    "#         vcurr=model_month_means['{:}_MONTH_MEANS'.format(d)]\n",
    "#         model_month_means['reanMonthMean']+=(vcurr.values)/float(n_rean)\n",
    "        \n",
    "#     for d in dataset_model:\n",
    "#         dcurr=flist_blobs2[d]\n",
    "#         d_mf = xa.open_mfdataset(dcurr)\n",
    "#         #Create a new blank variable\n",
    "#         mean_arr=np.zeros((len(time_list),len(new_lats),len(new_lons)))\n",
    "#         #String name \n",
    "#         v='{:}_MONTH_MEANS'.format(d)\n",
    "#         model_month_means[v]=xa.DataArray(mean_arr,dims=['time','lat','lon'])\n",
    "#         #Group the variable by months\n",
    "#         xtest=d_mf['Z_BLOB'].groupby('time.month')\n",
    "#         #Mean for the month in each of the years available\n",
    "#         for t in xtest.groups.keys():\n",
    "#             xyear=d_mf['Z_BLOB'][xtest.groups[t],:,:]\n",
    "#             yearmean=xyear.groupby('time.year').mean('time')\n",
    "#             yearmean_i=yearmean.interp(lat=new_lats,lon=new_lons).fillna(0)\n",
    "#             #Match to the correct time index by year and month\n",
    "#             yearlist=list(map(str,yearmean.year.values))\n",
    "#             inds=[y + '-{:02d}'.format(t) for y in yearlist]\n",
    "#             yearfin=xa.DataArray(yearmean_i.values,\n",
    "#                              coords={'lat':new_lats,\n",
    "#                                      'lon':new_lons,\n",
    "#                                      'time':inds},\n",
    "#                              dims=['time','lat','lon'])\n",
    "#             model_month_means[v].loc[inds,:,:]=yearfin             \n",
    "            \n",
    "#     return(model_month_means)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_test",
   "language": "python",
   "name": "jupyter_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
